\chapter{Variations on Normalisation by Evaluation}

Normalisation by Evaluation (NBE) is a technique leveraging the computational
power of a host language in order to normalise expressions of a deeply
embedded one. The process is based on a model construction describing a
family of types by induction on its \AF{Type} index. Two
procedures are then defined: the first (\AF{eval}) constructs an element
of \AB{ùìí} \AB{œÉ} \AB{Œì} provided a well typed term of the corresponding
\AD{Term} \AB{œÉ} \AB{Œì} type whilst the second (\AF{reify}) extracts, in
a type-directed manner, normal forms \AD{Nf} \AB{œÉ} \AB{Œì} from elements
of the model \AB{ùìí} \AB{œÉ} \AB{Œì}. NBE composes the two procedures. The
definition of this \AF{eval} function is a natural candidate for our
\AF{Semantics} framework. NBE is always defined \emph{for} a
given equational theory; we start by recalling the various
rules a theory may satisfy.

\subsection{Reduction Rules}

Thanks to \AF{Renaming} and \AF{Substitution} respectively, we can formally
define Œ∑-expansion and Œ≤-reduction. The Œ∑-rules say that for some types,
terms have a canonical form: functions will all be Œª-headed whilst records will
collect their fields -- here this makes all elements of \AIC{\unit{}} equal to \AIC{`one}.

\todo{insert definition of eta}

\begin{figure}[h]
\begin{mathpar}
\inferrule{\AB{t} ~\AS{:}~ \AD{Term}~ (\AB{œÉ} ~\AIC{`‚Üí}~ \AB{œÑ})~ \AB{Œì}
  }{\AB{t} \leadsto{} \AF{eta}~\AB{t}
  }{Œ∑_1}
\and \inferrule{\AB{t} ~\AS{:}~ \AD{Term}~ \AIC{`Unit}~ \AB{Œì}
  }{\AB{t} \leadsto{} \AIC{`one}
  }{Œ∑_2}
\and
\inferrule{
  }{\AIC{`app} ~(\AIC{`lam}~\AB{t})~ \AB{u} \leadsto \AB{t}~ \AF{‚ü®}~ \AB{u} ~\AF{/0‚ü©}
  }{Œ≤}
\end{mathpar}
\caption{Œ≤Œ∑ Rules for our Calculus\label{fig:betaetarules}}
\end{figure}

The Œ≤-rule is the main driver for actual computation,
but the presence of an inductive data type (\AIC{`Bool}) and its eliminator
(\AIC{`ifte}) means we have further redexes: whenever the
boolean the eliminator branches on is in canonical form, we may apply
a Œπ-rule. Finally, the Œæ-rule lets us reduce under
Œª-abstractions --- the distinction between weak-head normalisation and
strong normalisation.

\begin{figure}[h]
\begin{mathpar}
\inferrule{
  }{\AIC{`ifte}~\AIC{`tt}~\AB{l}~\AB{r} ~\leadsto{}~ \AB{l}
  }{Œπ_1}
\and
\inferrule{
  }{\AIC{`ifte}~\AIC{`ff}~\AB{l}~\AB{r} ~\leadsto{}~ \AB{r}
  }{Œπ_2}
\and
\inferrule{\AB{t} ~\leadsto{}~ \AB{u}
  }{\AIC{`lam}~ \AB{t} ~\leadsto{}~ \AIC{`lam}~\AB{u}
  }{Œæ}
\end{mathpar}
\caption{ŒπŒæ Rules for our Calculus\label{fig:betaetarules}}
\end{figure}

Now that we have recalled all these rules, we can talk precisely about the
sort of equational theory decided by the model construction we choose to
perform. We start with the usual definition of NBE
which goes under Œªs and produces Œ∑-long Œ≤Œπ-short normal forms.

\subsection{Normal and Neutral Forms}

We parametrise the mutually defined inductive families \AD{Ne} and \AD{Nf}
by a predicate \AB{R} constraining the types at which one may embed a neutral
as a normal form. This constraint shows up in the type of \AIC{`neu}; it makes
it possible to control whether the NBE should Œ∑-expands all terms at certain
types by prohibiting the existence of neutral terms at said type.

\begin{figure}[h]
\ExecuteMetaData[type-scope-semantics.agda/Syntax/Normal.tex]{normal}
\caption{Neutral and Normal Forms}
\end{figure}

Once more, the expected notions of thinning \AF{th\textasciicircum{}Ne} and
\AF{th\textasciicircum{}Nf} are induced as \AD{Ne} and \AD{Nf} are syntaxes.
We omit their purely
structural implementation here and wish we could do so in source code,
too: our constructions so far have been syntax-directed and could
surely be leveraged by a generic account of syntaxes with binding.
We will tackle this problem in~\cref{a-universe}.

\section{Normalisation by Evaluation for Œ≤ŒπŒæŒ∑}
\label{normbye}

In the case of NBE, the environment values and the computations in the model
will both use the same type family \AF{Model}, defined by induction on the
\AD{Type} argument. The Œ∑-rules allow us to represent functions (respetively
inhabitants of \AIC{`Unit}) in the source language as function spaces
(respectively values of type \AR{‚ä§}). Evaluating a \AIC{`Bool} may however
yield a stuck term so we can't expect the model to give us anything more than
an open term in normal form.

The model construction then follows the usual pattern pioneered by
Berger~(\citeyear{berger1993program}) and formally analysed and thoroughly
explained by Catarina Coquand~(\citeyear{coquand2002formalised}). We work
by induction on the type and describe Œ∑-expanded values: all inhabitants
of (\AF{Model} \AIC{`Unit} \AB{Œì}) are equal and all elements
of (\AF{Model} (\AB{œÉ} \AIC{`‚Üí} \AB{œÑ}) \AB{Œì}) are functions in Agda.

\begin{figure}[h]
\ExecuteMetaData[type-scope-semantics.agda/Semantics/NormalisationByEvaluation/BetaIotaXiEta.tex]{model}
\caption{Model for Normalisation by Evaluation\label{fig:nbemodel}}
\end{figure}

This model is defined by induction on the type in terms either of
syntactic objects (\AD{Nf}) or using the \AF{‚ñ°}-operator which is
a closure operator for Thinnings. As such, it is trivial to prove
that for all type \AB{œÉ}, (\AF{Model} \AB{œÉ}) is \AF{Thinnable}.

\begin{figure}[h]
\ExecuteMetaData[type-scope-semantics.agda/Semantics/NormalisationByEvaluation/BetaIotaXiEta.tex]{thmodel}
\caption{Values in the Model are Thinnable\label{fig:thnbemodel}}
\end{figure}

Application's semantic counterpart is easy to define: given that \AB{ùì•}
and \AB{ùìí} are equal in this instance definition we can feed the argument
directly to the function, with the identity renaming. This corresponds to
\AF{extract} for the comonad \AF{‚ñ°}.

\begin{figure}[h]
\ExecuteMetaData[type-scope-semantics.agda/Semantics/NormalisationByEvaluation/BetaIotaXiEta.tex]{app}
\caption{Semantic Counterpart of \AIC{`app}\label{fig:nbeapp}}
\end{figure}

Conditional branching however is more subtle: the boolean value \AIC{`if} branches on
may be a neutral term in which case the whole elimination form is stuck. This forces
us to define \AF{reify} and \AF{reflect} first. These functions, also known as quote
and unquote respectively, give the interplay between neutral terms, model values and
normal forms. \AF{reflect} performs a form of semantic Œ∑-expansion: all stuck \AIC{`Unit}
terms are equated and all functions are Œª-headed. It allows us to define \AF{var0}, the
semantic counterpart of (\AIC{`var} \AIC{z}).

\begin{figure}[h]
\ExecuteMetaData[type-scope-semantics.agda/Semantics/NormalisationByEvaluation/BetaIotaXiEta.tex]{reifyreflect}
\caption{Reify and Reflect\label{fig:nbeifte}}
\end{figure}

We can then give the semantics of \AIC{`ifte}: if the boolean is a value, the
appropriate branch is picked; if it is stuck then the whole expression is stuck.
It is then turned into a neutral form by reifying the two branches and then reflected
in the model.

\begin{figure}[h]
\ExecuteMetaData[type-scope-semantics.agda/Semantics/NormalisationByEvaluation/BetaIotaXiEta.tex]{ifte}
\caption{Semantic Counterpart of \AIC{`ifte}\label{fig:nbeifte}}
\end{figure}

We can then combine these components. The semantics of a Œª-abstraction is simply the
identity function: the structure of the functional case in the definition of the model
matches precisely the shape expected in a \AF{Semantics}. Because the environment
carries model values, the variable case is trivial.

\begin{figure}[h]
\ExecuteMetaData[type-scope-semantics.agda/Semantics/NormalisationByEvaluation/BetaIotaXiEta.tex]{eval}
\caption{Evaluation is a \AR{Semantics}\label{fig:evalnbe}}
\end{figure}

We can define a normaliser by kickstarting the evaluation with an environment of
placeholder values obtained by reflecting the term's free variables and then reifying
the result.

\begin{figure}[h]
\ExecuteMetaData[type-scope-semantics.agda/Semantics/NormalisationByEvaluation/BetaIotaXiEta.tex]{norm}
\caption{Normalisation as Reification of an Evaluated Term\label{fig:evalnbe}}
\end{figure}

\section{Normalisation by Evaluation for Œ≤ŒπŒæ}

As seen above, the traditional typed model construction leads to an NBE
procedure outputting Œ≤Œπ-normal Œ∑-long terms. However actual proof systems
rely on evaluation strategies that avoid applying Œ∑-rules
as much as possible: unsurprisingly, it is a rather bad idea to Œ∑-expand proof
terms which are already large when typechecking complex developments.

In these systems, normal forms are neither Œ∑-long nor Œ∑-short: the Œ∑-rule is
never deployed except when comparing a neutral and a constructor-headed term
for equality. Instead of declaring
them distinct, the algorithm does one step of Œ∑-expansion on the
neutral term and compares their subterms structurally. The conversion test
fails only when confronted with neutral terms with distinct head
variables or normal forms with different head constructors.

To reproduce this behaviour, NBE must be amended.
It is possible to alter the model definition described earlier so that it
avoids unnecessary Œ∑-expansions. We proceed by enriching the traditional
model with extra syntactical artefacts in a manner reminiscent of Coquand
and Dybjer's~(\citeyear{CoqDybSK}) approach to defining an NBE procedure for the SK combinator calculus. Their resorting to glueing
terms to elements of the model was dictated by the sheer impossibily to write
a sensible reification procedure but, in hindsight, it provides us with a
powerful technique to build models internalizing alternative equational
theories.

This leads us to using a predicate \AF{R} allowing embedding of neutrals
into normal forms at all types and mutually defining the model (\AF{Kr})
together with the \emph{acting} model (\AF{Go}).
