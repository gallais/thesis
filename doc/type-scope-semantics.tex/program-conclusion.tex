\chapter{Discussion}
\label{chapter:program-conclusion}

\section{Summary}

We have now seen how to give an intrinsically well-scoped and well-typed
presentation of a simple calculus. We represent it as an inductive family
indexed by the term's type and the context it lives in. Variables are
represented by typed de Bruijn indices.

To make the presentation lighter, we have made heavy use of a small domain
specific language to define indexed families. This allows us to silently
thread the context terms live in and only ever explicitly talk about it
when it gets extended.

We have seen a a handful of vital traversals such as thinning and substitution
which, now that they act on a well-typed and well-scoped syntax need to be
guaranteed to be type and scope preserving.

We have studied how McBride~(\citeyear{mcbride2005type}) identifies the
structure common to thinning and substitution, introduces a notion of \AR{Kit}
and refactors the two functions as instances of a more fundamental \AR{Kit}-based
traversal.

After noticing that other usual programs such as the evaluation function for
normalisation by evaluation seemed to fit a similar pattern, we have generalised
McBride's \AR{Kit} to obtain our notion of \AR{Semantics}. The accompanying
fundamental lemma is the core of this whole part. It demonstrates that
provided a notion of values and a notion of computations abiding by the
\AR{Semantics} constraints, we can write a scope-and-type preserving traversal
taking an environment of values, a term and returning a computation.

Thinning, substitution, normalisation by evaluation, printing with names, and
various continuation passing style translations are all instances of this
fundamental lemma.

\section{Related Work}

This part of the work which focuses on programming and not (yet!) on proving,
can be fully replicated in Haskell.
% and a translation of the definitions is available in the paper's repository~\cite{repo}
The subtleties of working with dependent types in Haskell (\cite{lindley2014hasochism})
are outside the scope of this paper.

If the tagless and typeful normalisation by evaluation procedure derived in Haskell
from our Semantics framework is to the best of our knowledge the first of its kind,
Danvy, Keller and Puech have achieved a similar goal in OCaml~(\citeyear{danvytagless}).
But their formalisation uses parametric higher order abstract syntax
(\cite{chlipala2008parametric})
freeing them from having to deal with variable binding, contexts and use models à
la Kripke at the cost of using a large encoding. However we find scope safety
enforced at the type level to be a helpful guide when formalising complex
type theories. It helps us root out bugs related to fresh name generation,
name capture or conversion from de Bruijn levels to de Bruijn indices.

This construction really shines in a simply typed setting but it is not
limited to it: we have successfully used an analogue of our Semantics
framework to enforce scope safety when implementing the expected traversals
(renaming, substitution, untyped normalisation by evaluation and printing
with names) for the untyped λ-calculus (for which the notion of type safety
does not make sense) or Martin-Löf type theory. Apart from NBE (which relies
on a non strictly-positive datatype), all of these traversals are total.

This work is at the intersection of two traditions: the formal treatment
of programming languages and the implementation of embedded Domain Specific
Languages (eDSL, \cite{hudak1996building}) both require the designer to
deal with name binding and the associated notions of renaming and substitution
but also partial evaluation (\cite{danvy1999type}), or even printing when
emitting code or displaying information back to the user (\cite{wiedijk2012pollack}).
The mechanisation of a calculus in a \emph{meta language} can use either
a shallow or a deep embedding (\cite{svenningsson2013combining,gill2014domain}).


The well-scoped and well typed final encoding described by Carette, Kiselyov,
and Shan~(\citeyear{carette2009finally}) allows the mechanisation of a calculus in
Haskell or OCaml by representing terms as expressions built up from the
combinators provided by a ``Symantics''. The correctness of the encoding
relies on parametricity~(\cite{reynolds1983types}) and although there exists
an ongoing effort to internalise parametricity~(\cite{bernardy2013type}) in
Martin-Löf Type Theory, this puts a formalisation effort out of the reach of all the
current interactive theorem provers.

Because of the strong restrictions on the structure our models may have,
we cannot represent all the interesting traversals imaginable. Chapman and
Abel's work on normalisation by evaluation
(\citeyear{chapman2009type,abel2014normalization})
which decouples the description of the big-step algorithm and its termination
proof is for instance out of reach for our system. Indeed, in their development
the application combinator may \emph{restart} the computation by calling the
evaluator recursively whereas the \ARF{app} constraint we impose means
that we may only combine induction hypotheses.

McBride's original unpublished work~(\citeyear{mcbride2005type}) implemented
in Epigram (\cite{mcbride2004view}) was inspired by Goguen and McKinna's
Candidates for Substitution (\citeyear{goguen1997candidates}). It focuses on
renaming and substitution for the simply typed $λ$-calculus and was later
extended to a formalisation of System F (\cite{girard1972interpretation})
in Coq (\cite{Coq:manual}) by Benton, Hur, Kennedy and McBride
(\citeyear{benton2012strongly}).
Benton et al. both implement a denotational semantics for their language
and prove the properties of their traversals. However both of these things
are done in an ad-hoc manner: the meaning function associated to their
denotational semantics is not defined in terms of the generic traversal
and the proofs are manually discharged one by one.

\section{Further Work}

There are three main avenues for future work and we will tackle all of these
later on this thesis. We could focus on the study of instances of \AR{Semantics},
the generalisation of \AR{Semantics} to a whole class of syntaxes with binding
rather than just our simple STLC, or proving properties of the traversals that
are instances of \AR{Semantics}.

\subsection{Other instances}

The vast array of traversals captured by this framework from meta-theoretical
results (stability under thinning and substitution) to programming tools
(printing with names) and compilation passes (partial evaluation and
continuation passing style translations) suggests that this method is widely
applicable. The quest of ever more traversals to refactor as instances of the
fundamental lemma of \AR{Semantics} is a natural candidate for further work.

We will see later on that once we start considering other languages including
variants with fewer invariants baked in, we can find new candidates. The fact
that erasure from a language with strong invariants to an untyped one falls
into this category may not be too surprising. The fact that the other direction,
that is type checking of raw terms or even elaboration of such raw terms to a
typed core language also corresponds to a notion of \AR{Semantics} is perhaps
more intriguing.

\subsection{A Generic Notion of Semantics}

If we look closely at the set of constraints a \AR{Semantics} imposes on the
notions of values and computations, we can see that it matches tightly the
structure of our language:

\begin{itemize}
  \item Each base constructor needs to be associated to a computation of the
    same type;
  \item Each eliminator needs to be interpreted as a function combining the
    interpretation of its subterms into the interpretation of the whole;
  \item The lambda case is a bit special: it uses a Kripke function space
    from values to computation as its interpretation
\end{itemize}

We can apply this recipe mechanically to enrich our language with e.g.
product and sum types, their constructor and eliminators. This suggests
that we ought to be able to give a generic description of syntaxes with
binding and the appropriate notion of Semantics for each syntax. We will
make this intuition precise in~\cref{a-universe}.

\subsection{Properties of Semantics}

Finally, because we know e.g. that we can prove generic theorems for all the
programs defined using fold (\cite{DBLP:journals/scp/Malcolm90}), the fact
that all of these traversals are instances of a common fold-like function
suggests that we ought to be able to prove general theorems about its
computational behaviour and obtain interesting results for each instance
as corollaries. This is the topic we will focus on for now.
